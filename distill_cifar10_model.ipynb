{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPHLXrwobwFuKND8XnOSvjn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinyanakashima/note-distillation-model/blob/main/distill_cifar10_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 概要\n",
        "CIFAR10モデルの知識蒸留を検証する。\n"
      ],
      "metadata": {
        "id": "EahrcL9-F9ED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "51x0kotpE9gz"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dURYoX7y7CcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なデータのロード\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. データローダーの設定\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# CIFAR-10データセットのロード\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# 2. モデルの設定\n",
        "import torchvision.models as models\n",
        "# 教師モデル（事前学習済みのResNet18）\n",
        "teacher_model = models.resnet18(pretrained=True)\n",
        "# 教師モデルは学習済みなので推論モードに\n",
        "teacher_model.eval()\n",
        "# CIFAR-10用に変更\n",
        "# 教師モデルの最終層をCIFAR-10の10クラスに置き換える\n",
        "num_ftrs = teacher_model.fc.in_features\n",
        "teacher_model.fc = torch.nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# 生徒モデル（ResNet18をベースにした簡略化）\n",
        "student_model = models.resnet18(pretrained=False)\n",
        "student_model.fc = torch.nn.Linear(student_model.fc.in_features, 10)  # CIFAR-10用に変更\n",
        "\n",
        "# 3. デバイス設定（GPUを使いたい場合）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher_model = teacher_model.to(device)\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "# 4. 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "# 最適化手法（例: Adam）\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "# 5. ハイパーパラメータの設定\n",
        "alpha = 0.1      # ソフトターゲット損失の重み\n",
        "T = 2.0          # 温度パラメータ\n",
        "\n",
        "# 6. 学習ループ\n",
        "for epoch in range(10):  # 例として10エポックでループ\n",
        "    for images, labels in train_loader:  # 学習データでループ\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # 1. 教師モデルと生徒モデルの予測を取得\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher_model(images)         # 教師モデルのロジット\n",
        "        student_logits = student_model(images)             # 生徒モデルのロジット\n",
        "\n",
        "        # 2. 教師モデルのロジットからソフトターゲット確率分布を計算\n",
        "        teacher_probs = F.softmax(teacher_logits / T, dim=1)\n",
        "\n",
        "        # 3. 生徒モデルのロジットにも温度Tを適用し、ソフトマックス -> distillation損失計算\n",
        "        student_log_probs = F.log_softmax(student_logits / T, dim=1)\n",
        "        distill_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
        "\n",
        "        # 4. 生徒モデルの通常のラベルに対するクロスエントロピー損失（ハードターゲット損失）\n",
        "        hard_loss = F.cross_entropy(student_logits, labels)\n",
        "\n",
        "        # 5. 二つの損失を合成\n",
        "        loss = alpha * distill_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "        # 6. 生徒モデルのパラメータを更新\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq6fLixnFTeg",
        "outputId": "82a85b0b-f208-476c-f9f2-3f51aeef8b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 71.2MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 173MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    }
  ]
}